# -*- coding: utf-8 -*-
"""Data_science_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1os62fFSq7wBiQtHNBuUzvYyTA0A8do7C
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler , MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import VotingRegressor
from sklearn import metrics
from sklearn.metrics import r2_score
import seaborn as sns

df=pd.read_csv("train-chennai-sale.csv")
df.head()

df.describe()
 df.shape
 df.dtypes

df.isnull().sum()



df.isnull().sum()

"""**FILLING NULL VALUES**"""

df['N_BEDROOM']=df['N_BEDROOM'].fillna(df['N_BEDROOM'].median())
df['N_BATHROOM']=df['N_BATHROOM'].fillna(df['N_BATHROOM'].median())
df['QS_OVERALL']=df['QS_OVERALL'].fillna(df['QS_OVERALL'].mean())

df

df.isnull().sum()

df.info()

"""**CHECKING FOR UNIQUE VALUES**"""

df['AREA'].unique()

df['AREA']=df['AREA'].replace({'Ann Nagar':'Anna Nagar','Karapakam':'Karapakkam','Velchery':'Velachery','Adyr':'Adyar','Chrmpet':'Chrompet','Chormpet':'Chrompet','Chrompt':'Chrompet','KKNagar':'KK Nagar','TNagar':'T Nagar','Ana Nagar':'Anna Nagar'})

df['AREA'].unique()

df['SALE_COND'].unique()

df['SALE_COND']=df['SALE_COND'].replace({'AbNormal':'Ab Normal','PartiaLl':'Partial','Partiall':'Partial','AdjLand':'Adj Land'})

df['SALE_COND'].unique()

df['PARK_FACIL'].unique()

df['PARK_FACIL']=df['PARK_FACIL'].replace({'Noo':'No'})

df['PARK_FACIL'].unique()

df['BUILDTYPE'].unique()

df['BUILDTYPE']=df['BUILDTYPE'].replace({'Comercial':'Commercial','Other':'Others'})

df['BUILDTYPE'].unique()

df['UTILITY_AVAIL'].unique()

df['UTILITY_AVAIL']=df['UTILITY_AVAIL'].replace({'AllPub':'All Pub','NoSeWa':'NoSewr '})
space=lambda x:x.strip()
df['UTILITY_AVAIL']=df['UTILITY_AVAIL'].apply(space)

df['UTILITY_AVAIL'].unique()

df['STREET'].unique()

df['STREET']=df['STREET'].replace({'Pavd':'Paved','NoAccess':'No Access'})

df['STREET'].unique()

df['MZZONE'].unique()

"""**EDA ON DATA**"""

df['AREA'].value_counts().plot(kind='bar')
plt.title('AREA OF VARIUOS PLACES')

df['SALE_COND'].value_counts().plot(kind='bar')
plt.title('SALE CONDITION')

df.columns

df['PARK_FACIL'].value_counts().plot(kind='bar')
plt.title('PARKING FACILITY')

df['UTILITY_AVAIL'].value_counts().plot(kind='bar')
plt.title('UTILITY AVAILABILITY ')

df['STREET'].value_counts().plot(kind='bar')
plt.title('STREET COUNTS')

df['MZZONE'].value_counts().plot(kind='bar')
plt.title('MZZONE COUNTS')

df['BUILDTYPE'].value_counts().plot(kind='bar')
plt.title('BUILDTYPE COUNTS')

A=df['AREA'].groupby(df['AREA']).count()
B=df['AREA'].groupby(df['BUILDTYPE']).count()
C=df['AREA'].groupby(df['UTILITY_AVAIL']).count()
D=df['AREA'].groupby(df['STREET']).count()
E=df['AREA'].groupby(df['MZZONE']).count()
F=df['AREA'].groupby(df['PARK_FACIL']).count()

sns.set_theme(style="darkgrid",palette="pastel")
plt.figure(figsize=(20,16))
plt.subplot(231)
sns.barplot(x=A.index,y=A.values,data=df)
plt.xticks(rotation=15)
plt.title('NO. OF HOUSES (AREA WISE)')
plt.subplot(232)
sns.barplot(x=B.index,y=B.values,data=df)
plt.xticks(rotation=15)
plt.title('NO. OF HOUSES (BUILDTYPE WISE)')
plt.subplot(233)
sns.barplot(x=C.index,y=C.values,data=df)
plt.xticks(rotation=15)
plt.title('NO. OF HOUSES (UTILITY_AVAIL WISE)')
plt.subplot(234)
sns.barplot(x=D.index,y=D.values,data=df)
plt.xticks(rotation=15)
plt.title('NO. OF HOUSES (STREET WISE)')
plt.subplot(235)
sns.barplot(x=E.index,y=E.values,data=df)
plt.xticks(rotation=15)
plt.title('NO. OF HOUSES (MZZONE)')
plt.subplot(236)
sns.barplot(x=F.index,y=F.values,data=df)
plt.xticks(rotation=15)
plt.title('NO. OF HOUSES (PARK_FACILITY)')
plt.show()

sns.set_theme(style="darkgrid",palette="pastel")
plt.figure(figsize=(20,16))
plt.subplot(231)
sns.barplot(x='AREA',y='SALES_PRICE',data=df,order=df.groupby('AREA')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['AREA'])
plt.xticks(rotation=15)
plt.title('MEAN SALES_PRICE (AREA WISE)')
plt.subplot(232)
sns.barplot(x='BUILDTYPE',y='SALES_PRICE',data=df,order=df.groupby('BUILDTYPE')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['BUILDTYPE'])
plt.xticks(rotation=15)
plt.title('MEAN SALES_PRICE (BUILDTYPE WISE)')
plt.subplot(233)
sns.barplot(x='UTILITY_AVAIL',y='SALES_PRICE',data=df,order=df.groupby('UTILITY_AVAIL')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['UTILITY_AVAIL'])
plt.xticks(rotation=15)
plt.title('MEAN SALES_PRICE (UTILITY_AVAIL WISE)')
plt.subplot(234)
sns.barplot(x='STREET',y='SALES_PRICE',data=df,order=df.groupby('STREET')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['STREET'])
plt.xticks(rotation=15)
plt.title('MEAN SALES_PRICE (STREET WISE)')
plt.subplot(235)
sns.barplot(x='PARK_FACIL',y='SALES_PRICE',data=df)
plt.xticks(rotation=15)
plt.title('MEAN SALES_PRICE (PARK_FACIL)')
plt.subplot(236)
sns.barplot(x='MZZONE',y='SALES_PRICE',data=df)
plt.xticks(rotation=15)
plt.title('MEAN SALES_PRICE (MZZONE)')
plt.show()

"""**ENCODING**"""

from sklearn.preprocessing import LabelEncoder

label=LabelEncoder()
df['BUILDTYPE']=label.fit_transform(df['BUILDTYPE'])
df['AREA']=label.fit_transform(df['AREA'])
df['PARK_FACIL']=label.fit_transform(df['PARK_FACIL'])
df['MZZONE']=label.fit_transform(df['MZZONE'])
df['SALE_COND']=label.fit_transform(df['SALE_COND'])
df['UTILITY_AVAIL']=label.fit_transform(df['UTILITY_AVAIL'])
df['STREET']=label.fit_transform(df['STREET'])

df.head()

"""We need to find the age of house which can be found with the difference of date build and date saled."""

df['Age']=pd.to_datetime(df['DATE_SALE'],format='%d-%m-%Y')-pd.to_datetime(df['DATE_BUILD'],format='%d-%m-%Y')
for i in df.index:
  df.loc[i,'Age']=df.loc[i,'Age'].days

df.head()

"""The column 'PRTID' can be removed, since it is just the individual id's of houses. And we can drop the 'REG_FEE' and 'COMMIS' as it can be found only when sale is made. Also we can drop the 'DATE_SALE' and 'DATE_BUILD' as we have already taken 'AGES'. Since it is mentioned in the decription QS featured are masked data, the columns 'QS_ROOMS',
'QS_BATHROOM', 'QS_BEDROOM', 'QS_OVERALL' can be dropped.
"""

df=df.drop(['PRT_ID','REG_FEE','COMMIS','DATE_SALE','DATE_BUILD','QS_ROOMS',
       'QS_BATHROOM', 'QS_BEDROOM', 'QS_OVERALL'],axis=1)
df

df.info()

"""the age is in object type, convert it into int64."""

df['Age']=df['Age'].apply(pd.to_numeric)
df.info()

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(),annot=True,cmap='YlGnBu')
plt.title('CORRELATION')

"""From the correlation it is evident that Sales price is more dependent on Number of rooms and  on  interior square feet  which has a correlation of 0.6 and 0.61"""

plt.figure(figsize=(10,10))
sns.scatterplot(x='SALES_PRICE',y='INT_SQFT',data=df,hue='INT_SQFT')
plt.title('SALES_PRICE VS INT_SQUARE_FEET')

plt.figure(figsize=(10,10))
sns.scatterplot(x='SALES_PRICE',y='N_ROOM',data=df,hue='N_ROOM')
plt.title('SALES_PRICE VS NUMBER OF ROOMS')

"""Splitting the data"""

x=df.drop(['SALES_PRICE'],axis=1)
y=df['SALES_PRICE']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)

"""Scaling"""

scale=StandardScaler()
normal_x_train=pd.DataFrame(scale.fit_transform(x_train),columns=x_train.columns)
normal_x_train

normal_x_test=pd.DataFrame(scale.fit_transform(x_test),columns=x_test.columns)
normal_x_test.head()

"""
**MODEL TRAINING**
         

        Now with the preprocessed data we need to build models and want to know which ML model gives best R^2 value. Here we have used the standard scaler data."""

d={}
def metrics_R2_score(actual,predicted,name):
  r2=r2_score(actual,predicted)
  d[name]=[r2]

def ml_model(x_test=None,y_test=None,model=None,name=''):
  predicted=model.predict(x_test)
  actual=y_test
  metrics_R2_score(actual,predicted,name)
  plt.figure(figsize=(15,10))
  plt.scatter(actual,predicted)
  p1 = max(max(predicted), max(actual))
  p2 = min(min(predicted), min(actual))

  plt.plot([p1,p2],[p1,p2])
  plt.xlabel('True Values')
  plt.ylabel('Predictions')
  plt.title(name)
  plt.axis('equal')
  plt.show()



"""**LINEAR REGRESSION**"""

lr=LinearRegression()
lr.fit(normal_x_train,y_train)
y_predict_lr=lr.predict(normal_x_test)
print('R2-Score :',metrics.r2_score(y_test,y_predict_lr))

ml_model(x_test=normal_x_test,y_test=y_test,model=lr,name='LINEAR REGRESSION')

"""**RANDOM FOREST REGRESSOR**"""

rfg=RandomForestRegressor(n_estimators=100,max_depth=15,max_features='sqrt')
rfg.fit(normal_x_train,y_train)
y_predict_rfg=rfg.predict(normal_x_test)
print('R2-Score :',metrics.r2_score(y_test,y_predict_rfg))

ml_model(x_test=normal_x_test,y_test=y_test,model=rfg,name="Random Forest Regression")

"""**DECISION TREE REGRESSION**"""

dt=DecisionTreeRegressor()
dt.fit(normal_x_train,y_train)
y_predict_dt=dt.predict(normal_x_test)
print('R2-Score :',metrics.r2_score(y_test,y_predict_dt))

ml_model(x_test=normal_x_test,y_test=y_test,model=dt,name="Decision Tree Regression")

final_result=pd.DataFrame(d,index=["R2"])
final_result

final_result.loc['R2'].max()

"""From the final result it is seen that Random Forest Regressor has the highest  R2 value of 0.98.

# **CONCLUSION:**
 For the above project we have done the cleaning of the data, removed the duplicates, done the data preprocessing, and found the correlation between various features which aids to the determination of the sales price in various areas, type of buildings which will give more sale price when sold etc.

It can be clearly seen that for a agent to build an building in chennai it is preferable to build a "commercial" type than other types and also it would be build in "T-Nagar" area , and also with all utilities and with parking facility. It is also evident that street way with gravel type is good choice. These are the best way that a agent can get more money/selling price which is evident from the data.
"""

